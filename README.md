# Awesome-Multimodal-Agents

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of multimodal AI agents and related resources. Focuses on systems that combine multiple input modalities (text, vision, speech, etc.) and demonstrate agentic capabilities like reasoning, tool use, and environment interaction.

## Contents
- [Core Concepts](#core-concepts)
- [Agent Frameworks](#agent-frameworks)
- [Research Papers](#papers)
  - [Surveys & Overviews](#surveys--overviews)
  - [Models & Architectures](#models--architectures)
  - [Tools & Agent Learning](#tools--agent-learning)
  - [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)
- [Tools & Libraries](#tools--libraries)
- [Tutorials & Courses](#tutorials--courses)
- [Contributing](#contributing)

## Core Concepts
- **Multimodal Understanding**: Processing and connecting information from multiple modalities
- **Agentic Behavior**: Autonomous decision-making, task completion, and environment interaction
- **Tool Augmentation**: Integration with external APIs, databases, and software tools
- **Embodied AI**: Agents operating in physical/virtual environments

## Agent Frameworks
[Previous content remains unchanged]

## Papers

### Surveys & Overviews
| Date       | Title                                                                 | Venue          | Paper | Code |
|------------|-----------------------------------------------------------------------|----------------|-------|------|
| 2024-02    | Large Multimodal Agents: A Survey                                     | arXiv          | [Paper](https://arxiv.org/abs/2402.15116) | - |
| 2024-01    | Agent AI: Surveying the Horizons of Multimodal Interaction            | arXiv          | [Paper](https://arxiv.org/abs/2401.03568) | - |
| 2023-12    | A Survey on Multimodal Large Language Models                          | ICLR 2024      | [Paper](https://arxiv.org/abs/2311.00201) | [Code](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) |
| 2023-08    | The Rise and Potential of Large Language Model Based Agents           | NeurIPS 2023   | [Paper](https://arxiv.org/abs/2309.07864) | - |

### Models & Architectures
| Date       | Title                                                                 | Venue          | Paper | Code |
|------------|-----------------------------------------------------------------------|----------------|-------|------|
| 2023-07    | PaLM-E: An Embodied Multimodal Language Model                         | ICML 2023      | [Paper](https://arxiv.org/abs/2303.03378) | [Code](https://github.com/google-research/palm-e) |
| 2023-06    | Visual Instruction Tuning                                             | NeurIPS 2023   | [Paper](https://arxiv.org/abs/2304.08485) | [Code](https://github.com/haotian-liu/LLaVA) |
| 2023-03    | Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models | ICLR 2024  | [Paper](https://arxiv.org/abs/2303.04671) | [Code](https://github.com/microsoft/visual-chatgpt) |
| 2023-02    | Multimodal Chain-of-Thought Reasoning in Language Models              | ACL 2023       | [Paper](https://arxiv.org/abs/2302.00923) | [Code](https://github.com/amazon-research/mm-cot) |

### Tools & Agent Learning
| Date       | Title                                                                 | Venue          | Paper | Code |
|------------|-----------------------------------------------------------------------|----------------|-------|------|
| 2023-09    | Tool Learning with Foundation Models                                  | NeurIPS 2023   | [Paper](https://arxiv.org/abs/2304.08354) | [Code](https://github.com/OpenBMB/ToolBench) |
| 2023-08    | Language Models as Zero-Shot Tool Learners                           | ICLR 2024      | [Paper](https://arxiv.org/abs/2308.00676) | - |
| 2023-06    | SPRING: Situated Planning and Reasoning for Instruction Following     | ICML 2023      | [Paper](https://arxiv.org/abs/2305.11346) | - |
| 2023-05    | ReAct: Synergizing Reasoning and Acting in Language Models           | ICLR 2023      | [Paper](https://arxiv.org/abs/2210.03629) | [Code](https://github.com/ysymyth/ReAct) |

### RAG (Retrieval-Augmented Generation)
| Date       | Title                                                                 | Venue          | Paper | Code |
|------------|-----------------------------------------------------------------------|----------------|-------|------|
| 2024-02    | Self-RAG: Learning to Retrieve, Generate and Iterate                  | ICLR 2024      | [Paper](https://arxiv.org/abs/2310.11511) | [Code](https://github.com/AkariAsai/self-rag) |
| 2023-10    | Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models | AAAI 2024  | [Paper](https://arxiv.org/abs/2311.09210) | - |
| 2023-08    | REALM: Retrieval-Augmented Language Model Pre-Training               | ICML 2023      | [Paper](https://arxiv.org/abs/2002.08909) | [Code](https://github.com/google-research/language/tree/master/language/realm) |
| 2023-06    | Atlas: Few-shot Learning with Retrieval Augmented Language Models    | ICML 2023      | [Paper](https://arxiv.org/abs/2208.03299) | [Code](https://github.com/facebookresearch/atlas) |

[Rest of the content remains unchanged]
