# Awesome-Multimodal-Agents

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A curated list of multimodal AI agents and related resources. Focuses on systems that combine multiple input modalities (text, vision, speech, etc.) and demonstrate agentic capabilities like reasoning, tool use, and environment interaction.

## Contents
- [Core Concepts](#core-concepts)
- [Agent Frameworks](#agent-frameworks)
- [Research Papers](#papers)
- [Datasets](#datasets)
- [Tools & Libraries](#tools--libraries)
- [Tutorials & Courses](#tutorials--courses)
- [Contributing](#contributing)

## Core Concepts
- **Multimodal Understanding**: Processing and connecting information from multiple modalities
- **Agentic Behavior**: Autonomous decision-making, task completion, and environment interaction
- **Tool Augmentation**: Integration with external APIs, databases, and software tools
- **Embodied AI**: Agents operating in physical/virtual environments

## Agent Frameworks

| Framework | Description | Language | License |
|-----------|-------------|----------|---------|
| [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) | Experimental open-source autonomous AI agent | Python | MIT |
| [LangChain](https://github.com/langchain-ai/langchain) | Building context-aware reasoning applications | Python | MIT |
| [Transformers Agent](https://huggingface.co/docs/transformers/transformers_agents) | Natural language interface for 100+ ML models | Python | Apache 2.0 |

## Papers

| Date       | Title                                                                 | Venue          | Paper | Code |
|------------|-----------------------------------------------------------------------|----------------|-------|------|
| 2023-10-12 | GPT-4V(ision) System Card                                            | arXiv          | [Paper](https://cdn.openai.com/papers/GPTV_System_Card.pdf) | - |
| 2023-07-20 | PaLM-E: An Embodied Multimodal Language Model                         | ICML 2023      | [Paper](https://arxiv.org/abs/2303.03378) | [Code](https://github.com/google-research/palm-e) |
| 2023-03-30 | Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models | arXiv        | [Paper](https://arxiv.org/abs/2303.04671) | [Code](https://github.com/microsoft/visual-chatgpt) |

## Datasets

| Name | Modalities | Description | Size | Link |
|------|------------|-------------|------|------|
| COCO | Image+Text | Common Objects in Context | 330K images | [Website](https://cocodataset.org/) |
| Visual Genome | Image+Graph | Dense visual annotations | 108K images | [Website](https://visualgenome.org/) |
| Something-Something | Video+Text | Temporal action understanding | 220K videos | [Website](https://20bn.com/datasets/something-something) |

## Tools & Libraries

| Tool | Description | Language | Link |
|------|-------------|----------|------|
| HuggingFace Transformers | State-of-the-art NLP models | Python | [GitHub](https://github.com/huggingface/transformers) |
| PyTorch Lightning | ML research framework | Python | [Website](https://www.pytorchlightning.ai/) |
| FiftyOne | Dataset visualization | Python | [Website](https://voxel51.com/fiftyone/) |

## Contributing

Contributions welcome! Please:
1. Ensure entries are ordered chronologically
2. Verify all links are working
3. Maintain consistent formatting
4. Add new categories through issues first

License: [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)
